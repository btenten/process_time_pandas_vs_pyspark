{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyspark\n",
        "import os\n",
        "import seaborn as sns\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    path = r'/Users/ten/Documents/Python Scripts/pd_vs_pyspark'\n",
        "    os.chdir(path)\n",
        "except:\n",
        "    pass\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "print('Spark session created.')\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pd_import_file():\n",
        "    start_time = time.time()\n",
        "    df = pd.read_csv(f\"{path}/PS_20174392719_1491204439457_log.csv\")\n",
        "    total_run_time = time.time() - start_time\n",
        "    return total_run_time\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pyspark_import_file():\n",
        "    start_time = time.time()\n",
        "    df = spark.read.csv(f\"{path}/PS_20174392719_1491204439457_log.csv\", header = True, inferSchema = True)\n",
        "    total_run_time = time.time() - start_time\n",
        "    return total_run_time\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pd_filter_groupby(df_pd):\n",
        "    start_time1 = time.time()\n",
        "    df_pd_filter = df_pd[(df_pd['type'].isin(['CASH_OUT', 'TRANSFER'])) & (df_pd['amount'] >= 100000)]\n",
        "    filter_runtime = time.time()-start_time1\n",
        "\n",
        "    start_time2 = time.time()\n",
        "    df_pd_groupby = df_pd.groupby(['type'])['isFraud', 'isFlaggedFraud'].sum()\n",
        "    groupby_runtime = time.time()-start_time2\n",
        "\n",
        "    return filter_runtime, groupby_runtime\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pyspark_filter_groupby(df_pyspark):\n",
        "    start_time1 = time.time()\n",
        "    df_pyspark_filter = spark.sql(\"\"\"Select *\n",
        "                                        From transactions\n",
        "                                        Where type in('CASH_OUT', 'TRANSFER')\n",
        "                                        and amount >= 100000\"\"\")\n",
        "    #df_pyspark_filter.show()\n",
        "    filter_runtime = time.time()-start_time1\n",
        "\n",
        "    start_time2 = time.time()\n",
        "    df_pyspark_groupby = spark.sql(\"\"\"Select\n",
        "                                        type,\n",
        "                                        sum(isFraud) as total_is_fraud,\n",
        "                                        sum(isFlaggedFraud) as total_is_flagged_fraud\n",
        "                                      From transactions\n",
        "                                      Group by 1\"\"\")\n",
        "    #df_pyspark_groupby.show()\n",
        "    groupby_runtime = time.time()-start_time2\n",
        "\n",
        "    return filter_runtime, groupby_runtime\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_read_csv_time = pd.DataFrame(columns = ['loop_num', 'pandas', 'pyspark'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,50):\n",
        "    print(f\"Start loop number {i}..\")\n",
        "    df_read_csv_time = df_read_csv_time.append({'loop_num': i,\n",
        "                                                'pandas': pd_import_file(),\n",
        "                                                'pyspark': pyspark_import_file()},\n",
        "                                                ignore_index = True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_read_csv_time.describe()\n",
        "print(f\"Avg read_csv time (sec):\\n--pandas: {df_read_csv_time.describe()['pandas']['mean']}\\n--pyspark: {df_read_csv_time.describe()['pyspark']['mean']}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filter_groupby_time = pd.DataFrame(columns = ['loop_num', 'pandas_filter', 'pandas_groupby', 'pyspark_filter', 'pyspark_groupby'])\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd_ori = pd.read_csv(f\"{path}/PS_20174392719_1491204439457_log.csv\")\n",
        "df_pyspark_ori = spark.read.csv(f\"{path}/PS_20174392719_1491204439457_log.csv\", header = True, inferSchema = True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 50):\n",
        "    df_pd = df_pd_ori.copy()\n",
        "    df_pyspark = df_pyspark_ori\n",
        "    df_pyspark.createOrReplaceTempView('transactions')\n",
        "\n",
        "    print(f\"Start loop | pandas | number {i}..\")\n",
        "    pd_filter_runtime, pd_groupby_runtime = pd_filter_groupby(df_pd)\n",
        "\n",
        "    print(f\"Start loop | pyspark | number {i}..\")\n",
        "\n",
        "    pyspark_filter_runtime, pyspark_groupby_runtime = pyspark_filter_groupby(df_pyspark)\n",
        "\n",
        "    df_filter_groupby_time = df_filter_groupby_time.append({'loop_num': i,\n",
        "                                                'pandas_filter': pd_filter_runtime,\n",
        "                                                'pandas_groupby': pd_groupby_runtime,\n",
        "                                                'pyspark_filter': pyspark_filter_runtime,\n",
        "                                                'pyspark_groupby': pyspark_groupby_runtime},\n",
        "                                                ignore_index = True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sns.lineplot(data = df_read_csv_time, x = 'loop_num', y = 'pandas')\n",
        "_ = sns.lineplot(data = df_read_csv_time, x = 'loop_num', y = 'pyspark')\n",
        "_.set_title('read_csv run time | pandas vs pyspark')\n",
        "_.set_ylabel('Run time (sec)')\n",
        "_.set_xlabel('Loop number')\n",
        "_.legend(['pandas', 'pyspark'])\n",
        "plt.show()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sns.lineplot(data = df_filter_groupby_time, x = 'loop_num', y = 'pandas_filter')\n",
        "_ = sns.lineplot(data = df_filter_groupby_time, x = 'loop_num', y = 'pyspark_filter')\n",
        "_.set_title('filter run time | pandas vs pyspark')\n",
        "_.set_ylabel('Run time (sec)')\n",
        "_.set_xlabel('Loop number')\n",
        "_.legend(['pandas', 'pyspark'])\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = sns.lineplot(data = df_filter_groupby_time, x = 'loop_num', y = 'pandas_groupby')\n",
        "_ = sns.lineplot(data = df_filter_groupby_time, x = 'loop_num', y = 'pyspark_groupby')\n",
        "_.set_title('groupby run time | pandas vs pyspark')\n",
        "_.set_ylabel('Run time (sec)')\n",
        "_.set_xlabel('Loop number')\n",
        "_.legend(['pandas', 'pyspark'])\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "argv": [
        "/opt/anaconda3/bin/python",
        "-m",
        "ipykernel_launcher",
        "-f",
        "{connection_file}"
      ],
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}